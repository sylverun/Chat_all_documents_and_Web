{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af044d8d-956b-4c88-bb51-e3cbc4aef94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f1bca-6db2-4919-9c56-8c243fab3140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de8170-d581-4fcc-89fc-896c19af2c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2dc344-523d-4a06-a4ce-24add701740d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82a4c9-2d7b-4bf3-8de6-5262944a10ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319ec90-97b0-4b20-a55b-451c99f01d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b603ac-a560-48c0-9aa2-8ccb46f46fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader= DirectoryLoader(r'C:\\Users\\sylv_\\Desktop\\Test_AI_Chat',glob=\"./*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea003e-638e-4ed5-a228-6e0a449af871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (documents[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7048e8b-40eb-419a-9b7d-b13960e01b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter()\n",
    "chunks_docs= text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83365083-d0ba-4f8c-abfa-0d1704e0e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "len(chunks_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda77d8-651b-4d40-9b77-0bcc10b0e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store= Chroma.from_documents(chunks_docs, HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d99df-941c-4d86-b802-176ffea43dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= vec_store.as_retriever()\n",
    "#create the chain to answer questions\n",
    "qa_chain_instrucEmb= RetrievalQA.from_chain_type(llm=HuggingFaceEndpoint(), \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b6de3-4095-4ad8-8dde-3b5b18809fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "        \n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a2aec-f089-43e6-9206-44e19183a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"'who are the authors of Distinct differences in rates of oxygen consumption and ATP synthesis?\"\n",
    "docs = vec_store.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f719ec7-6479-416d-aa11-89418f3fd3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75e472-dd15-4a10-930c-7c052c33af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ed7a5-ba38-4c41-aac7-4a223f195a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", model_kwargs={\"temperature\":0.4})\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac17652-7cae-4152-8456-2722f2060a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to cite the paper called Distinct differences in rates of oxygen consumption and ATP synthesis?\" \n",
    "docs = vec_store.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883fb61-6621-4d8d-a9e0-74a7c32961c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065fe67-3fba-46a5-9c77-d48c61e4068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def generate_pet_name(animal_type, couleur):\n",
    "    llm= HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.5})\n",
    "    \n",
    "    prompt_template_name= PromptTemplate(\n",
    "        input_variables=['animal_type', 'couleur'],template=\" J'ai {animal_type} qui est de couleur {couleur} et je veux lui donner un nom cool. Donne moi une liste de 5 noms cools pour mon animal.\")\n",
    "    name_chain= LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"pet_name\")\n",
    "    response = name_chain.invoke({'animal_type': animal_type, 'couleur': couleur})\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_pet_name(\"un chien\", \"marron\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4717ec-9b97-4114-8a16-5dd98e4dda30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cad68-cd9f-4c10-87a0-61775aca9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95102f-b35e-48cd-b29a-c39fc2d42639",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92814841-9b53-4141-9205-6b87a6e51752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bcad8c-8f0e-4974-9606-c900a875beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477a782-1b7c-4fe4-8a5c-b391d11e3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db_from_youtube_url(video_url:str)-> Chroma:\n",
    "    loader= YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript= loader.load()\n",
    "    \n",
    "    text_splitter= RecursiveCharacterTextSplitter()\n",
    "    docs= text_splitter.split_documents(transcript)\n",
    "    \n",
    "    db= Chroma.from_documents(docs, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e22c8-51de-4e6a-8e62-3821cbe229a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url= \"https://www.youtube.com/watch?v=lG7Uxts9SXs\"\n",
    "create_vector_db_from_youtube_url(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade39316-f76a-44f9-89b8-6a027310e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_query(db, query,k=25):\n",
    "    #the mixtralmodel can handle 32k tokens to define k max= nbrtokens/chunk_size\n",
    "    \n",
    "    docs= db.similarity_search(query, k=k)\n",
    "    docs_page_content= \" \".join([d.page_content for d in docs])\n",
    "    \n",
    "    llm= HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.2})\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"docs\"],\n",
    "        template=\"\"\"\n",
    "        You are a helpful assistant that that can answer questions about youtube videos \n",
    "        based on the video's transcript.\n",
    "        \n",
    "        Answer the following question: {question}\n",
    "        By searching the following video transcript: {docs}\n",
    "        \n",
    "        Only use the factual information from the transcript to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "        \n",
    "        Your answers should be verbose and detailed.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response=chain.run(question=query, docs=docs_page_content)\n",
    "    #response=chain.invoke({'question': query, 'docs': docs_page_content})\n",
    "    response=response.replace(\"\\n\",\" \")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738c7a9-5da6-4dee-9382-a596b5c254ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db=create_vector_db_from_youtube_url(video_url)\n",
    "test_query=\" What is the purpose of this talk\"\n",
    "print(get_response_from_query(test_db, test_query,k=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1debf56-62ef-4323-9ad5-3bb52f95d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac032128-16b0-47a0-80c1-ecae3672c784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafa454-a77c-49b3-b725-fb23771a8374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165bd5fd-c53f-4cb3-8a7b-ddc32c9c2519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever,create_retrieval_chain\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_vectorstore_from_URL(url):\n",
    "    # get the text in doc format\n",
    "    loader= WebBaseLoader(url)\n",
    "    documents=loader.load()\n",
    "    \n",
    "    #split documents\n",
    "    text_splitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks_docs= text_splitter.split_documents(documents)\n",
    "\n",
    "    #create the vectorstore\n",
    "    embeddings= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "    vec_store= Chroma.from_documents(chunks_docs, embeddings)\n",
    "    return vec_store\n",
    "    \n",
    "def get_context_retriever_chain(vectorstore):\n",
    "    llm= HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.2})\n",
    "    retriever= vectorstore.as_retriever()\n",
    "    prompt= ChatPromptTemplate.from_messages([\n",
    "        (\"human\",\"{input}\"),\n",
    "        (\"human\",\"Given the above conversation, generate a search query to look up in order to get informations relevant to the conversation.\"),\n",
    "    ])\n",
    "    \n",
    "    retriever_chain= create_history_aware_retriever(llm,retriever,prompt)\n",
    "    \n",
    "    return retriever_chain\n",
    "\n",
    "def get_conversational_rag_chain(retriever_chain):\n",
    "    llm= HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.2})\n",
    "    prompt= ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "        (\"user\",\"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain=create_stuff_documents_chain(llm,prompt)\n",
    "    \n",
    "    return create_retrieval_chain(retriever_chain,stuff_documents_chain)\n",
    "\n",
    "\n",
    "def extract_response(text):\n",
    "    # Trouver l'index où \"Assistant:\" apparaît\n",
    "    index = text.find(\"Assistant:\")\n",
    "    \n",
    "    # Extraire la partie après \"Assistant:\"\n",
    "    if index != -1:\n",
    "        return text[index + len(\"Assistant:\"):]\n",
    "    else:\n",
    "        return \"Pas de réponse trouvée après 'Assistant:'\"\n",
    "\n",
    "def get_response(user_in):   \n",
    "    retriever_chain= get_context_retriever_chain(vec_store)\n",
    "    conversation_rag_chain = get_conversational_rag_chain(retriever_chain)\n",
    "    \n",
    "    response=conversation_rag_chain.invoke({\n",
    "        \"input\":user_in\n",
    "        })\n",
    "    \n",
    "    texte_complet = response[\"answer\"]\n",
    "    reponse_extraite = extract_response(texte_complet)\n",
    "    \n",
    "    return reponse_extraite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88748e15-ed4f-48b1-99d6-364dddd1f1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM (Large Language Model) applications. It extends the LangChain library, allowing you to coordinate multiple chains or actors across multiple steps of computation in a cyclic manner. This enables more complex, agent-like behaviors where you can call an LLM in a loop.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_store= get_vectorstore_from_URL(\"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\")\n",
    "retriever_chain= get_context_retriever_chain(vec_store)\n",
    "conversation_rag_chain = get_conversational_rag_chain(retriever_chain)\n",
    "    \n",
    "get_response(\"What is langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4579e60-9a00-40ff-9cec-3d9d62efc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_QA={}\n",
    "\n",
    "default_QA={\n",
    "    \"What is the capital of France?\": \"Paris\",\n",
    "    \"Who wrote 'Romeo and Juliet'?\": \"William Shakespeare\",\n",
    "    \"What is the tallest mountain in the world?\": \"Mount Everest\",\n",
    "    \"What is the chemical symbol for water?\": \"H2O\",\n",
    "    \"Who painted the Mona Lisa?\": \"Leonardo da Vinci\",\n",
    "    \"What is the currency of Japan?\": \"Yen\",\n",
    "    \"What is the powerhouse of the cell?\": \"Mitochondria\",\n",
    "    \"What is the boiling point of water in Celsius?\": \"100\",\n",
    "    \"Who is the first president of the United States?\": \"George Washington\",\n",
    "    \"What is the largest ocean on Earth?\": \"Pacific Ocean\"\n",
    "}\n",
    "\n",
    "def obtain_listes(dictionnaire):\n",
    "    cles = []\n",
    "    valeurs = []\n",
    "    for cle, valeur in dictionnaire.items():\n",
    "        cles.append(cle)\n",
    "        valeurs.append(valeur)    \n",
    "    return cles, valeurs\n",
    "\n",
    "def Running_Test(dico_QA=default_QA):\n",
    "    nbr_of_Question= len(dico_QA)\n",
    "    list_of_questions, list_of_answers= obtain_listes(dico_QA)\n",
    "    score=0\n",
    "    for i in range (0, nbr_of_Question):\n",
    "        answer= input(list_of_questions[i] +\"\\n\")\n",
    "        if answer.lower() == list_of_answers[i].lower():\n",
    "            print(\"Correct!\\n\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(\"Incorrect!\\n\")\n",
    "    percent= (score/nbr_of_Question)*100\n",
    "    print(f\"You have {score}/{nbr_of_Question} ({percent}%) of correct answers!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9693769-5705-46dd-bd5a-f86836546a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      " Paris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who wrote 'Romeo and Juliet'?\n",
      " William Shakespeare\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the tallest mountain in the world?\n",
      " fjds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the chemical symbol for water?\n",
      " fnhf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who painted the Mona Lisa?\n",
      " fgjsnfn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the currency of Japan?\n",
      " gjifko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the powerhouse of the cell?\n",
      " gkig\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the boiling point of water in Celsius?\n",
      " gfg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Who is the first president of the United States?\n",
      " gl,s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What is the largest ocean on Earth?\n",
      " Pacific ocean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n",
      "\n",
      "You have 3/10 (30.0%) of correct answers!!!\n"
     ]
    }
   ],
   "source": [
    "Running_Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286fa447-53d5-43bc-83fb-6bbc71912357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What is the formula for water?': 'H2O',\n",
       " 'What is the charge of a chlorine ion?': '-1',\n",
       " 'What is the atomic number of oxygen?': '8',\n",
       " 'What is the name of the compound NaCl?': 'table salt'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "Type_of_questions=[\"Closed(True or False) question\", \"Recall question\"]\n",
    "list_Level= [\"beginner\",\"intermediate\",\"expert\"]\n",
    "\n",
    "def generate_dico_QA(number_of_questions, domain, level):\n",
    "    repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    llm= HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0.3})\n",
    "    \n",
    "    prompt_template_name= PromptTemplate(\n",
    "        input_variables=['number_of_questions','domain','level'],\n",
    "        template=\" You are a helpful assistant who can develop questions to test knowledge. Generate {number_of_questions} questions relating to the domain of {domain} and for a {level} level.The answers don't exceed three word.You will format the questions as the keys and the answers as the values of a python dictionary.\")\n",
    "    name_chain= LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "    response = name_chain.run({'number_of_questions': number_of_questions, 'domain':domain, 'level': level})\n",
    "    index_debut = response.find(\"{\")\n",
    "    index_fin = response.rfind(\"}\") + 1\n",
    "    dictionnaire_str = response[index_debut:index_fin]\n",
    "    try:\n",
    "        dictionnaire = ast.literal_eval(dictionnaire_str)\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(\"Erreur lors de l'extraction du dictionnaire :\", e)\n",
    "    return dictionnaire\n",
    "\n",
    "dico=generate_dico_QA(\"4\", \"chemistry\", \"intermediate\")\n",
    "dico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
